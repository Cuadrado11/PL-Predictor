{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30112093-1a0b-48c1-9c1f-d5a323d22af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "############################################################################################################################################\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import json_normalize\n",
    "import requests\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import joblib #FOR HOLDOUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4760e48e-2da6-4e5e-abac-858746a8e8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'id': 16924773, 'sport_id': 1, 'league_id': 8, 'season_id': 17420, 'stage_id': 77448322, 'group_id': None, 'aggregate_id': None, 'round_id': 201980, 'state_id': 5, 'venue_id': 230, 'name': 'Liverpool vs Burnley', 'starting_at': '2021-01-21 20:00:00', 'result_info': 'Burnley won after full-time.', 'leg': '1/1', 'details': None, 'length': 90, 'placeholder': False, 'has_odds': True, 'has_premium_odds': False, 'starting_at_timestamp': 1611259200}, 'subscription': [{'meta': {'trial_ends_at': None, 'ends_at': '2025-07-03 19:28:14', 'current_timestamp': 1750979094}, 'plans': [{'plan': 'Charles Camarena Custom Plan 2', 'sport': 'Football', 'category': 'Custom'}], 'add_ons': [], 'widgets': []}], 'rate_limit': {'resets_in_seconds': 3359, 'remaining': 2998, 'requested_entity': 'Fixture'}, 'timezone': 'UTC'}\n"
     ]
    }
   ],
   "source": [
    "# API URL Base Checker\n",
    "url = 'https://api.sportmonks.com/v3/football/fixtures/16924773?api_token=GSoPMqoZ5e1lPovCnMOtOGlGmsREDwUP17vg54phbLQEX4OGNGDT7IDmF1ic'\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer YOUR_API_KEY',\n",
    "    'Content-Type': 'application/json'\n",
    "}\n",
    "\n",
    "# Send a GET request to the API\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the response JSON\n",
    "    data = response.json()\n",
    "    \n",
    "    # Print the filtered DataFrame\n",
    "    print(data)\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, Response: {response.text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83e9cb84-0332-4213-8dd6-3b3c4a18229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    season starting_at  index_column\n",
      "0       11  2009-08-15             0\n",
      "1        2  2010-08-14             1\n",
      "2        9  2011-08-13             2\n",
      "3        7  2012-08-18             3\n",
      "4        3  2013-08-17             4\n",
      "5       12  2014-08-16             5\n",
      "6       10  2015-08-08             6\n",
      "7       13  2016-08-13             7\n",
      "8     6397  2017-08-11             8\n",
      "9    12962  2018-08-10             9\n",
      "10   16036  2019-08-09            10\n",
      "11   17420  2020-09-12            11\n",
      "12   18378  2021-08-13            12\n",
      "13   19734  2022-08-05            13\n",
      "14   21646  2023-08-11            14\n"
     ]
    }
   ],
   "source": [
    "# GET SEASONS IN ORDER BY DATE\n",
    "############################################################################################################################################\n",
    "\n",
    "season_ids = [\n",
    "    2, 3, 7, 9, 10, 11, 12, 13, 6397, \n",
    "    12962, 16036, 17420, 18378, 19734, 21646 \n",
    "    # ,23614 # 2024 season\n",
    "    # ,24255, 24256, 24261, 24262, 24263 #These Correspond to very early seasons\n",
    "]\n",
    "dates_df = []\n",
    "# Loop over each season ID and construct the URL\n",
    "for season_id in season_ids:\n",
    "    url = f'https://api.sportmonks.com/v3/football/seasons/{season_id}?api_token=GSoPMqoZ5e1lPovCnMOtOGlGmsREDwUP17vg54phbLQEX4OGNGDT7IDmF1ic'\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer YOUR_API_KEY',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful (status code 200 means success)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Pretty-print the JSON data\n",
    "        pretty_data = json.dumps(data, indent=4)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "    json_data = json.loads(pretty_data)\n",
    "    # Use json_normalize to extract id and name from nested structure\n",
    "    df = json_normalize(json_data['data'], sep='_')\n",
    "    \n",
    "    # Selecting only the required fields\n",
    "    df_filt = df[['id', 'starting_at']]\n",
    "    df_filt.columns = ['season', 'starting_at']\n",
    "    dates_df.append(df_filt)\n",
    "date_df = pd.concat(dates_df, ignore_index=True)\n",
    "date_df = date_df.sort_values(by = 'starting_at', ignore_index=True)\n",
    "date_df['index_column'] = date_df.index\n",
    "print(date_df)\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d59375c3-0bbc-45dd-92ff-5dc478e42bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   season    id  gwnum                                  name  \\\n",
      "0       2   465      1        Aston Villa vs West Ham United   \n",
      "1       2  5504      2   West Ham United vs Bolton Wanderers   \n",
      "2       2  5452      3  Manchester United vs West Ham United   \n",
      "3       2  5313      4            West Ham United vs Chelsea   \n",
      "4       2  5102      5         Stoke City vs West Ham United   \n",
      "\n",
      "                 start  team_id  points home/away  opponent  opponent_pts  \\\n",
      "0  2010-08-14 14:00:00        1       0      away        15             3   \n",
      "1  2010-08-21 14:00:00        1       0      home        16             3   \n",
      "2  2010-08-28 16:30:00        1       0      away        14             3   \n",
      "3  2010-09-11 14:00:00        1       0      home        18             3   \n",
      "4  2010-09-18 11:45:00        1       1      away        26             1   \n",
      "\n",
      "   sum_smaller_gw  opponent_sum  form  opponent_form  fixture_id  away_goals  \\\n",
      "0               0             3   NaN            NaN         465           0   \n",
      "1               0             4   NaN            NaN        5504           3   \n",
      "2               0             7   NaN            NaN        5452           0   \n",
      "3               0            12   NaN            NaN        5313           3   \n",
      "4               1             4   NaN            NaN        5102           1   \n",
      "\n",
      "   home_goals  \n",
      "0           3  \n",
      "1           1  \n",
      "2           3  \n",
      "3           1  \n",
      "4           1  \n"
     ]
    }
   ],
   "source": [
    "# GET FIXTURE DATA\n",
    "# FINAL DF INCLUDES TEAM STATS, OPPONENT STATS, OTHER FEATURES\n",
    "############################################################################################################################################\n",
    "combined_df = pd.read_csv('combined_df.csv') #Just Headers for our df we are gonna build\n",
    "full_final = []\n",
    "\n",
    "\n",
    "# Loop over each season ID and construct the URL\n",
    "count = 0\n",
    "for season_id in season_ids:\n",
    "    count += 1\n",
    "    # url = f'https://api.sportmonks.com/v3/football/teams/seasons/{season_id}?api_token=GSoPMqoZ5e1lPovCnMOtOGlGmsREDwUP17vg54phbLQEX4OGNGDT7IDmF1ic'\n",
    "    url = f'https://api.sportmonks.com/v3/football/seasons/{season_id}?api_token=GSoPMqoZ5e1lPovCnMOtOGlGmsREDwUP17vg54phbLQEX4OGNGDT7IDmF1ic'\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer YOUR_API_KEY',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful (status code 200 means success)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Pretty-print the JSON data\n",
    "        pretty_data = json.dumps(data, indent=4)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "\n",
    "    \n",
    "    json_data = json.loads(pretty_data)\n",
    "    # Use json_normalize to extract id and name from nested structure\n",
    "    df = json_normalize(json_data['data'], sep='_')\n",
    "    \n",
    "    # Selecting only the required fields\n",
    "    df_filtered = df[['id', 'name']]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df_filtered.columns = ['id', 'name']\n",
    "\n",
    "    \n",
    "    # Define the API endpoint and any parameters or headers if needed\n",
    "    url = f'https://api.sportmonks.com/v3/football/rounds/seasons/{season_id}?api_token=GSoPMqoZ5e1lPovCnMOtOGlGmsREDwUP17vg54phbLQEX4OGNGDT7IDmF1ic'\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': 'Bearer YOUR_API_KEY',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful (status code 200 means success)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()\n",
    "        \n",
    "        # Pretty-print the JSON data\n",
    "        pretty_data = json.dumps(data, indent=4)\n",
    "        # print(pretty_data)\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "    \n",
    "    json_data = json.loads(pretty_data)\n",
    "    # Use json_normalize to extract id and name from nested structure\n",
    "    df = json_normalize(json_data['data'], sep='_')\n",
    "    # Selecting only the required fields\n",
    "    df_filtered = df[['id', 'starting_at', 'ending_at', 'season_id']]\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    df_filtered.columns = ['round_id', 'start', 'end', 'season']\n",
    "    df_round = df_filtered.copy()\n",
    "    df_gws = df_filtered.sort_values(by=['start'])\n",
    "    df_gws.rename(columns={'start': 'start_of_gw'}, inplace=True)\n",
    "\n",
    "    df_gws.reset_index(drop=True, inplace=True)\n",
    "    df_gws.index = df_gws.index + 1  # Add 1 to each index\n",
    "    \n",
    "    # Reset the index and keep the current index (1-38) as a new column named 'gwnum'\n",
    "    df_gws = df_gws.reset_index()\n",
    "    \n",
    "    # Rename the index column to 'gwnum'\n",
    "    df_gws.rename(columns={'index': 'gwnum'}, inplace=True)\n",
    "    \n",
    "    # Define the API endpoint and any parameters or headers if needed\n",
    "    url = f'https://api.sportmonks.com/v3/football/seasons/{season_id}?api_token=GSoPMqoZ5e1lPovCnMOtOGlGmsREDwUP17vg54phbLQEX4OGNGDT7IDmF1ic&include=fixtures.participants'\n",
    "    \n",
    "    headers = {\n",
    "        'Authorization': 'Bearer YOUR_API_KEY',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful (status code 200 means success)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()\n",
    "        if count == 1:\n",
    "            pretty_data = json.dumps(data, indent=4)\n",
    "            # print(pretty_data)\n",
    "        # Assuming df_round is your DataFrame with a column named 'round_id'\n",
    "        round_ids = df_round['round_id'].tolist()  # Convert the 'round_id' column to a list\n",
    "        \n",
    "        # Get the fixtures from the data\n",
    "        fixtures = data['data']['fixtures']\n",
    "        \n",
    "        # Filter fixtures by checking if the 'round_id' is in the round_ids list\n",
    "        filtered_fixtures = [fixture for fixture in fixtures if fixture['round_id'] in round_ids]\n",
    "\n",
    "\n",
    "\n",
    "        # Now, flatten the participants from the filtered fixtures\n",
    "        df = json_normalize(\n",
    "            filtered_fixtures,\n",
    "            record_path='participants',  # Only pull data from participants\n",
    "            meta=['id', 'round_id', 'name', 'starting_at'],  # Include 'name' and 'starting_at'\n",
    "            record_prefix='participant_'\n",
    "        )\n",
    "\n",
    "        \n",
    "        # # Now, flatten the participants from the filtered fixtures\n",
    "        # df = json_normalize(\n",
    "        #     filtered_fixtures, \n",
    "        #     record_path='participants',  # Only pull data from participants\n",
    "        #     meta=['id', 'round_id'],\n",
    "        #     record_prefix='participant_'\n",
    "        # )\n",
    "        \n",
    "        # Select relevant columns\n",
    "        df = df[['id', 'round_id', 'name', 'starting_at', 'participant_id', 'participant_meta.location', 'participant_meta.winner']]\n",
    "        # Rename columns for clarity\n",
    "        df.rename(columns={\n",
    "            'starting_at': 'start',\n",
    "            'participant_id': 'team_id',\n",
    "            'participant_meta.location': 'home/away',\n",
    "            'participant_meta.winner': 'winner',\n",
    "        }, inplace=True)\n",
    "        \n",
    "        # Group by fixture to determine ties (sum of winners per fixture)\n",
    "        df_grouped = df.groupby('id')['winner'].sum().reset_index()\n",
    "        \n",
    "        # Calculate result points\n",
    "        df['points'] = df['winner'].apply(lambda x: 3 if x else 0)\n",
    "        df.loc[df['id'].isin(df_grouped[df_grouped['winner'] == 0]['id']), 'points'] = 1  # For ties\n",
    "        df_result = df[['id', 'round_id', 'name', 'start', 'team_id', 'points', 'home/away']]\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "    \n",
    "    df_result.sort_values(by='round_id')\n",
    "    ############################################################################################################################################\n",
    "\n",
    "    # print(df_gws.head())\n",
    "    full_df = pd.merge(df_gws, df_result, on='round_id')\n",
    "    # print(full_df.columns)\n",
    "    ############################################################################################################################################\n",
    "\n",
    "    full_df = full_df[['season', 'id', 'gwnum', 'name', 'start', 'team_id', 'points', 'home/away']]\n",
    "    \n",
    "    # Create a new column 'opponent' by grouping by 'id' and assigning the opponent team_id\n",
    "    full_df['opponent'] = full_df.groupby('id')['team_id'].transform(lambda x: x.iloc[::-1].values)\n",
    "    full_df['opponent_pts'] = full_df.groupby('id')['points'].transform(lambda x: x.iloc[::-1].values)\n",
    "    \n",
    "    home_df = full_df.copy()\n",
    "\n",
    "    # Define a function to calculate the cumulative sum of scores for both the team and the opponent\n",
    "    def cumulative_sum_team_and_opponent(row, df):\n",
    "        # Calculate cumulative sum of points for the team\n",
    "        team_points_sum = df[(df['team_id'] == row['team_id']) & (df['gwnum'] <= row['gwnum'])]['points'].sum()\n",
    "        \n",
    "        # Calculate cumulative sum of points for the opponent\n",
    "        opponent_points_sum = df[(df['team_id'] == row['opponent']) & (df['gwnum'] <= row['gwnum'])]['points'].sum()\n",
    "        \n",
    "        return team_points_sum, opponent_points_sum\n",
    "    \n",
    "    # Apply the function and create two new columns for team's and opponent's cumulative points\n",
    "    home_df[['sum_smaller_gw', 'opponent_sum']] = home_df.apply(\n",
    "        lambda row: pd.Series(cumulative_sum_team_and_opponent(row, home_df)), axis=1\n",
    "    )\n",
    "\n",
    "    \n",
    "    index_df = home_df.copy()\n",
    "    index_df = index_df.reset_index(drop=True)\n",
    "    index_df.index.name = 'order' \n",
    "    # team_33_df.sort_values(by=['gwnum', 'id'])\n",
    "    index_df =  index_df.sort_values(by=['team_id','gwnum'])\n",
    "    index_df['form'] = index_df.groupby('team_id')['sum_smaller_gw'].transform(lambda x: x - x.shift(5))\n",
    "    # Step 1: Calculate the form for each team as we did before\n",
    "    # Create a temporary DataFrame with form calculation for each team\n",
    "    team_form_df = index_df[['team_id', 'gwnum', 'sum_smaller_gw']].copy()\n",
    "    team_form_df['team_form'] = team_form_df.groupby('team_id')['sum_smaller_gw'].transform(lambda x: x - x.shift(5))\n",
    "    \n",
    "    # Step 2: Rename columns in the temporary DataFrame to indicate it's the opponent's form\n",
    "    team_form_df = team_form_df.rename(columns={'team_id': 'opponent', 'team_form': 'opponent_form', 'gwnum': 'gwnum_opponent'})\n",
    "    \n",
    "    # Step 3: Merge the opponent's form back into the main DataFrame based on opponent ID and game week\n",
    "    index_df = index_df.merge(team_form_df[['opponent', 'gwnum_opponent', 'opponent_form']],\n",
    "                              left_on=['opponent', 'gwnum'],\n",
    "                              right_on=['opponent', 'gwnum_opponent'],\n",
    "                              how='left')\n",
    "    \n",
    "    # Step 4: Drop the auxiliary column 'gwnum_opponent' as it's no longer needed\n",
    "    index_df = index_df.drop(columns=['gwnum_opponent'])\n",
    "    \n",
    "    # Display the resulting DataFrame with the new 'opponent_form' column\n",
    "    # print(index_df.head(10))\n",
    "\n",
    "    url = f'https://api.sportmonks.com/v3/football/seasons/{season_id}?api_token=GSoPMqoZ5e1lPovCnMOtOGlGmsREDwUP17vg54phbLQEX4OGNGDT7IDmF1ic&include=fixtures.scores'\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': 'Bearer YOUR_API_KEY',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    # Send a GET request to the API\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Check if the request was successful (status code 200 means success)\n",
    "    if response.status_code == 200:\n",
    "        # Parse the response JSON\n",
    "        data = response.json()\n",
    "        # Pretty-print the JSON data\n",
    "        pretty_data = json.dumps(data, indent=4)\n",
    "    # Extract the fixtures\n",
    "    fixtures = data[\"data\"][\"fixtures\"]\n",
    "    \n",
    "    # Create a list to store the data for the DataFrame\n",
    "    current_scores_data = []\n",
    "    \n",
    "    # Iterate over each fixture to get the \"CURRENT\" scores\n",
    "    for fixture in fixtures:\n",
    "        for score in fixture[\"scores\"]:\n",
    "            if score[\"description\"] == \"CURRENT\":\n",
    "                current_scores_data.append({\n",
    "                    \"fixture_id\": fixture[\"id\"],\n",
    "                    \"team\": score[\"score\"][\"participant\"],\n",
    "                    \"goals\": score[\"score\"][\"goals\"]\n",
    "                })\n",
    "    \n",
    "    # Convert to a DataFrame\n",
    "    df_current_scores = pd.DataFrame(current_scores_data)\n",
    "    \n",
    "    # Pivot the data to have home and away goals in separate columns\n",
    "    df_pivoted = df_current_scores.pivot(index='fixture_id', columns='team', values='goals').reset_index()\n",
    "    \n",
    "    # Rename columns to make them more descriptive\n",
    "    df_pivoted.columns = ['fixture_id', 'away_goals', 'home_goals']\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    # print(df_pivoted)\n",
    "    merged = index_df.merge(df_pivoted, left_on = 'id', right_on = 'fixture_id', how='inner', suffixes=('',''))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    final_df = merged.copy()\n",
    "    # print(final_df)\n",
    "    full_final.append(final_df)\n",
    "    \n",
    "    full_final_df = pd.concat(full_final, ignore_index=True)\n",
    "    if season_id == 21646:\n",
    "        last_season_list = merged['id'].unique().tolist()\n",
    "\n",
    "# Create a list of unique values from the \"id\" column\n",
    "fixture_list = full_final_df['id'].unique().tolist()\n",
    "\n",
    "# Print the list (optional)\n",
    "# print(fixture_list)\n",
    "print(full_final_df.head())\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02b72f4e-3f4b-45b5-8dee-a975848dc6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RANDOM DATE FILTER TO AVOID NA/S\n",
    "# Convert 'start' to datetime if it isn't already\n",
    "full_final_df['start'] = pd.to_datetime(full_final_df['start'])\n",
    "\n",
    "# Filter for fixtures after a specific date\n",
    "filtered_df = full_final_df[(full_final_df['start'] > '2017-08-01')]\n",
    "\n",
    "# Get unique fixture IDs\n",
    "fix_list = filtered_df['fixture_id'].unique().tolist()\n",
    "\n",
    "# # Optional: print number and first few\n",
    "# print(f\"Total fixtures after 2015-08-01: {len(fix_list)}\")\n",
    "# filtered_df = full_final_df[~full_final_df['season'].isin([19734, 21646])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11af6cfa-667f-429f-82b0-ed1b4d6e7980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bookmakers to include\n",
    "# bookmaker_ids = {1, 2, 3}\n",
    "\n",
    "# # Hold results\n",
    "# all_data = []\n",
    "# no_data_fixtures = []\n",
    "\n",
    "# # Loop over each fixture\n",
    "# for i, fixture_id in enumerate(fix_list, start=1):\n",
    "    \n",
    "#     if i in {1, 10, 100, 400, 1000, 2000}:\n",
    "#         print(f\"\\n📍Checkpoint — Processing fixture #{i} (fixture_id: {fixture_id})\")\n",
    "\n",
    "#     url = f'https://api.sportmonks.com/v3/football/odds/pre-match/fixtures/{fixture_id}/markets/1?api_token=GSoPMqoZ5e1lPovCnMOtOGlGmsREDwUP17vg54phbLQEX4OGNGDT7IDmF1ic&select=bookmaker_id,probability,label,fixture_id'\n",
    "\n",
    "#     response = requests.get(url)\n",
    "\n",
    "#     if response.status_code == 200:\n",
    "#         data = response.json()\n",
    "\n",
    "#         if 'data' in data and data['data']:\n",
    "#             entries = data['data']\n",
    "#             df = pd.DataFrame(entries)\n",
    "\n",
    "#             # Filter only bookmaker 1, 2, 3\n",
    "#             df = df[df['bookmaker_id'].isin(bookmaker_ids)]\n",
    "\n",
    "#             if df.empty:\n",
    "#                 print(f\"❌ No data for fixture {fixture_id} from bookmakers 1,2,3\")\n",
    "#                 no_data_fixtures.append(fixture_id)\n",
    "#                 continue\n",
    "\n",
    "#             # Map labels\n",
    "#             label_map = {'1': 'win_p', 'X': 'draw_p', '2': 'loss_p'}\n",
    "#             df['label'] = df['label'].map(label_map)\n",
    "\n",
    "#             # Drop any unrecognized labels\n",
    "#             df = df[df['label'].notna()]\n",
    "\n",
    "#             # Convert percentages if needed\n",
    "#             df['probability'] = df['probability'].replace('%', '', regex=True).astype(float)\n",
    "\n",
    "#             # Pivot: rows = bookmaker_id, columns = label\n",
    "#             pivot_df = df.pivot_table(index='bookmaker_id', columns='label', values='probability', aggfunc='first')\n",
    "\n",
    "#             # Average across available bookmakers\n",
    "#             averaged = pivot_df.mean(skipna=True)\n",
    "\n",
    "#             # If all values are NaN, skip fixture\n",
    "#             if averaged.isna().all():\n",
    "#                 print(f\"⚠️ All 3 bookmakers missing data for fixture {fixture_id}\")\n",
    "#                 no_data_fixtures.append(fixture_id)\n",
    "#                 continue\n",
    "\n",
    "#             # Add to main list\n",
    "#             row = {\n",
    "#                 'fixture_id': fixture_id,\n",
    "#                 'win_p': averaged.get('win_p', None),\n",
    "#                 'draw_p': averaged.get('draw_p', None),\n",
    "#                 'loss_p': averaged.get('loss_p', None),\n",
    "#             }\n",
    "#             all_data.append(row)\n",
    "\n",
    "#         else:\n",
    "#             print(f\"❌ No 'data' in API response for fixture {fixture_id}\")\n",
    "#             no_data_fixtures.append(fixture_id)\n",
    "\n",
    "#     else:\n",
    "#         print(f\"❌ Failed to fetch fixture {fixture_id} — status {response.status_code}\")\n",
    "#         no_data_fixtures.append(fixture_id)\n",
    "\n",
    "# # Final DataFrame\n",
    "# result_df = pd.DataFrame(all_data)\n",
    "\n",
    "# # Show result\n",
    "# print(\"\\n🎯 Final Result DataFrame:\")\n",
    "# print(result_df)\n",
    "\n",
    "# # Show skipped fixtures\n",
    "# print(f\"\\n🛑 Fixtures with no data: {len(no_data_fixtures)}\")\n",
    "# print(no_data_fixtures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7f62073-b89b-4779-9189-df394eb7b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_df = pd.read_csv('od.csv')\n",
    "odds_df = odds_df.rename(columns={\n",
    "    'fixture_id': 'id',\n",
    "    'win_p': 'home_win',\n",
    "    'draw_p': 'draw',\n",
    "    'loss_p': 'away_win'\n",
    "})\n",
    "odds_df['home_win'] = odds_df['home_win']/ 100\n",
    "odds_df['draw'] = odds_df['draw']/ 100\n",
    "odds_df['away_win'] = odds_df['away_win']/ 100\n",
    "\n",
    "# df_merged = full_final_df[['name', 'id']].merge(odds_df\n",
    "# print(full_final_df.head())\n",
    "# print(odds_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f50c42c-d2a6-4868-a814-98f334175db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOME STRENGTH FEATURE PREVIOUS SEASONS\n",
    "############################################################################################################################################\n",
    "merged_full_df = full_final_df.merge(date_df, on='season', how='inner', suffixes=('', ''))\n",
    "\n",
    "# Define a function to calculate home and away points for a team in the previous season\n",
    "def calculate_home_strength(team_id, previous_season_df):\n",
    "    # Filter for games where the team played at home and calculate the total points\n",
    "    home_points = previous_season_df[(previous_season_df['team_id'] == team_id) & (previous_season_df['home/away'] == 'home')]['points'].sum()\n",
    "    \n",
    "    # Filter for games where the team played away and calculate the total points\n",
    "    away_points = previous_season_df[(previous_season_df['team_id'] == team_id) & (previous_season_df['home/away'] == 'away')]['points'].sum()\n",
    "    \n",
    "    # Calculate the custom feature score as the difference\n",
    "    return home_points - away_points\n",
    "\n",
    "# Initialize a column for the home strength score\n",
    "merged_full_df['home_strength_score'] = None\n",
    "\n",
    "# Loop through each unique season (index_column)\n",
    "for index_value in merged_full_df['index_column'].unique():\n",
    "    # Check if there is a previous season\n",
    "    if index_value == 0:\n",
    "        # For the first season, set home_strength_score to 0\n",
    "        merged_full_df.loc[merged_full_df['index_column'] == index_value, 'home_strength_score'] = 0\n",
    "        continue\n",
    "    \n",
    "    # Define the current and previous season data based on index_column\n",
    "    current_season_df = merged_full_df[merged_full_df['index_column'] == index_value]\n",
    "    previous_season_df = merged_full_df[merged_full_df['index_column'] == index_value - 1]\n",
    "    \n",
    "    # Calculate the home strength score for each team in the previous season\n",
    "    team_home_strength = {}\n",
    "    for team_id in previous_season_df['team_id'].unique():\n",
    "        team_home_strength[team_id] = calculate_home_strength(team_id, previous_season_df)\n",
    "    \n",
    "    # Assign the home strength score to each row of the current season for the corresponding team\n",
    "    def assign_home_strength(row):\n",
    "        return team_home_strength.get(row['team_id'], 0)  # Default to 0 if no previous season data\n",
    "    \n",
    "    merged_full_df.loc[merged_full_df['index_column'] == index_value, 'home_strength_score'] = current_season_df.apply(assign_home_strength, axis=1)\n",
    "\n",
    "# Print the updated DataFrame with home_strength_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33ae00a-b76e-4965-bb07-d3e3ef17b19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HOME STRENGTH FEATURE CURRENT SEASON\n",
    "############################################################################################################################################\n",
    "# Define a function to calculate cumulative home strength up to the current game week within the same season\n",
    "def calculate_cumulative_home_strength(row, df):\n",
    "    # Filter for games up to the current game week within the same season for the given team\n",
    "    team_id = row['team_id']\n",
    "    season = row['season']\n",
    "    gw = row['gwnum']\n",
    "    \n",
    "    # Get all games up to the current game week within the same season for the team\n",
    "    past_games = df[(df['season'] == season) & (df['team_id'] == team_id) & (df['gwnum'] < gw)]\n",
    "    \n",
    "    # Calculate cumulative home and away points\n",
    "    home_games = past_games[past_games['home/away'] == 'home']\n",
    "    away_games = past_games[past_games['home/away'] == 'away']\n",
    "    \n",
    "    # Calculate the total points for home and away games\n",
    "    home_points = home_games['points'].sum()\n",
    "    away_points = away_games['points'].sum()\n",
    "    \n",
    "    # Calculate the average points per game for home and away games\n",
    "    home_avg = home_points / len(home_games) if len(home_games) > 0 else 0\n",
    "    away_avg = away_points / len(away_games) if len(away_games) > 0 else 0\n",
    "    \n",
    "    # Calculate the custom feature score as the difference\n",
    "    return home_avg - away_avg\n",
    "\n",
    "# Initialize a new column for the cumulative home strength score within the current season\n",
    "merged_full_df['home_strength_score2'] = merged_full_df.apply(calculate_cumulative_home_strength, axis=1, df=merged_full_df)\n",
    "\n",
    "# Print the updated DataFrame with both home_strength_score and home_strength_score2\n",
    "# print(merged_full_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd3ca7b-2aa0-41cc-a49c-be37b64bf2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GD FEATURE\n",
    "############################################################################################################################################\n",
    "merged_full_df['goals_for'] = 0\n",
    "merged_full_df['goals_against'] = 0\n",
    "\n",
    "# Define a function to calculate cumulative goals for and goals against\n",
    "def calculate_goals(row, df):\n",
    "    team_id = row['team_id']\n",
    "    season = row['season']\n",
    "    gwnum = row['gwnum']\n",
    "    home_or_away = row['home/away']\n",
    "    \n",
    "    # Filter for matches within the same season and before the current game week for the team\n",
    "    past_matches = df[(df['season'] == season) & \n",
    "                      (df['gwnum'] < gwnum) & \n",
    "                      (df['team_id'] == team_id)]\n",
    "\n",
    "    # Calculate goals_for\n",
    "    goals_for = past_matches.apply(\n",
    "        lambda x: x['home_goals'] if x['home/away'] == 'home' else x['away_goals'], axis=1\n",
    "    ).sum()\n",
    "    \n",
    "    # Calculate goals_against\n",
    "    goals_against = past_matches.apply(\n",
    "        lambda x: x['away_goals'] if x['home/away'] == 'home' else x['home_goals'], axis=1\n",
    "    ).sum()\n",
    "    \n",
    "    return pd.Series([goals_for, goals_against])\n",
    "\n",
    "# Apply the function to each row\n",
    "merged_full_df[['goals_for', 'goals_against']] = merged_full_df.apply(calculate_goals, axis=1, df=merged_full_df)\n",
    "merged_full_df['gd'] = merged_full_df['goals_for'] - merged_full_df['goals_against']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0263e2d3-60f3-4a66-8bdd-a7a41bce447c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique seasons with NaN entries: []\n"
     ]
    }
   ],
   "source": [
    "# GW 15 - 38 FILTER\n",
    "############################################################################################################################################\n",
    "# print(full_final_df.head(10))\n",
    "full_final_df = merged_full_df\n",
    "\n",
    "# Initialize an empty list to collect DataFrames for each gameweek\n",
    "combined_dfs = []\n",
    "\n",
    "# Get the maximum gameweek number\n",
    "max_gw = full_final_df['gwnum'].max()\n",
    "\n",
    "############################################################################################################################################\n",
    "for season_id in season_ids:\n",
    "    for n in range(15, max_gw + 1):\n",
    "        # Get data for the current and previous gameweeks\n",
    "        current_season_df = full_final_df[full_final_df['season'] == season_id].copy()\n",
    "\n",
    "        gw_n_df = current_season_df[current_season_df['gwnum'] == n].copy()\n",
    "        gw_n_minus1_df = current_season_df[current_season_df['gwnum'] == n - 1].copy()\n",
    "\n",
    "        # Ensure gw_n_minus1_df has unique team_id rows\n",
    "        gw_n_minus1_team = gw_n_minus1_df[['team_id', 'sum_smaller_gw', 'form']].drop_duplicates(subset=['team_id'])\n",
    "        gw_n_minus1_team = gw_n_minus1_team.rename(columns={\n",
    "            'sum_smaller_gw': 'sum_smaller_gw_team_prev_gw',\n",
    "            'form': 'form_team_prev_gw'\n",
    "        })\n",
    "        \n",
    "        # Merge team data from the previous gameweek\n",
    "        gw_n = gw_n_df.merge(gw_n_minus1_team, on='team_id', how='left')\n",
    "\n",
    "        # Ensure unique opponent entries in previous gameweek\n",
    "        gw_n_minus1_opp = gw_n_minus1_df[['team_id', 'sum_smaller_gw']].drop_duplicates(subset=['team_id']).rename(\n",
    "            columns={'team_id': 'opponent', 'sum_smaller_gw': 'sum_smaller_gw_opponent_prev_gw'}\n",
    "        )\n",
    "        \n",
    "        # Merge opponent data from the previous gameweek\n",
    "        gw_n = gw_n.merge(gw_n_minus1_opp, left_on='opponent', right_on='opponent', how='left')\n",
    "\n",
    "        # Append processed data\n",
    "        combined_dfs.append(gw_n)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "combined_df = pd.concat(combined_dfs, ignore_index=True)\n",
    "# Select rows where any column is NaN\n",
    "nan_entries = combined_df[combined_df.isna().any(axis=1)]\n",
    "\n",
    "# Filter rows with any NaN entries and get unique 'season' values\n",
    "nan_seasons = combined_df[combined_df.isna().any(axis=1)]['season'].unique()\n",
    "\n",
    "# Print the unique seasons with NaN entries\n",
    "print(\"Unique seasons with NaN entries:\", nan_seasons)\n",
    "\n",
    "# print(nan_entries)\n",
    "\n",
    "# Remove rows with any NaN values\n",
    "# print(combined_df)\n",
    "combined_df = combined_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39c56e4d-35ab-46e2-b77f-f83cab354b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('allfeat.csv')\n",
    "dfcurr = pd.read_csv('allfeat.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fd05cca-4e92-485b-8e32-dc088e06ef9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature weights for each outcome class:\n",
      "                                     Loss      Draw       Win\n",
      "gd                              -0.064697 -0.047178  0.111874\n",
      "form_team_prev_gw                0.016569  0.007358 -0.023927\n",
      "sum_smaller_gw_team_prev_gw     -0.367376 -0.002731  0.370107\n",
      "sum_smaller_gw_opponent_prev_gw  0.435980 -0.050217 -0.385763\n",
      "home/away                       -0.217272 -0.024400  0.241672\n",
      "home_strength_feature2          -0.035483  0.020248  0.015235\n",
      "home_strength_feature           -0.031662  0.026923  0.004738\n"
     ]
    }
   ],
   "source": [
    "####PLAY AROUND###### (Looking at both home and away fixture)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Prepare the data (already partially done)\n",
    "df = combined_df.copy()\n",
    "\n",
    "# Convert 'points' into three classes: 2 = win (3 points), 1 = draw (1 point), 0 = loss (0 points)\n",
    "df['outcome'] = df['points']  # Use 'points' directly (3 -> win, 1 -> draw, 0 -> loss)\n",
    "df['outcome'] = df['points'].map({3: 2, 1: 1, 0: 0})  # Map 3 to 2, 1 to 1, and 0 to 0\n",
    "\n",
    "\n",
    "# Encode 'home/away' as a binary variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['home/away'] = label_encoder.fit_transform(df['home/away'])\n",
    "# df = df[df['home/away'] == 1]  # Only keep rows for home team\n",
    "\n",
    "# Add home-specific features\n",
    "df['home_strength_feature'] = df.apply(lambda row: row['home_strength_score'] if row['home/away'] == 1 else 0, axis=1)\n",
    "df['home_strength_feature2'] = df.apply(lambda row: row['home_strength_score2'] if row['home/away'] == 1 else 0, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Split into training and holdout sets\n",
    "# Exclude specified seasons from training\n",
    "# exclude_seasons = [18378, 21646, 19734]  # List of seasons to exclude\n",
    "exclude_seasons = [21646]  # List of seasons to exclude\n",
    "df = df[~df['season'].isin(exclude_seasons)].copy()\n",
    "\n",
    "\n",
    "# season_of_interest = 17420  # Future season for testing\n",
    "season_of_interest = 19734  # Future season for testing\n",
    "\n",
    "\n",
    "\n",
    "holdout_df = df[df['season'] == season_of_interest]  # Test data\n",
    "train_df = df[df['season'] != season_of_interest]  # Training data\n",
    "\n",
    "# Step 3: Define features and target for model training\n",
    "# features = ['gd', 'form_team_prev_gw', 'sum_smaller_gw_team_prev_gw', 'sum_smaller_gw_opponent_prev_gw', 'home/away', 'home_strength_feature2', 'home_strength_feature', 'opponent_form']\n",
    "features = ['gd', 'form_team_prev_gw', 'sum_smaller_gw_team_prev_gw', 'sum_smaller_gw_opponent_prev_gw', 'home/away', 'home_strength_feature2', 'home_strength_feature']\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['outcome']\n",
    "\n",
    "# Step 4: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Step 5: Train the multinomial logistic regression model\n",
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=200)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# View feature weights (coefficients) for each class (Loss, Draw, Win)\n",
    "import pandas as pd\n",
    "\n",
    "feature_weights_df = pd.DataFrame(\n",
    "    model.coef_,\n",
    "    columns=features,\n",
    "    index=['Loss', 'Draw', 'Win']\n",
    ")\n",
    "\n",
    "print(\"Feature weights for each outcome class:\")\n",
    "print(feature_weights_df.T)  # Transpose for readability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 6: Predict probabilities on the holdout set\n",
    "X_holdout = holdout_df[features]\n",
    "y_holdout = holdout_df['outcome']\n",
    "X_holdout_scaled = scaler.transform(X_holdout)\n",
    "\n",
    "holdout_df = holdout_df.copy()  # Ensure it's a full copy before editing\n",
    "\n",
    "holdout_df.loc[:, 'Model_Prob_Loss'] = y_holdout_pred_proba[:, 0]\n",
    "holdout_df.loc[:, 'Model_Prob_Draw'] = y_holdout_pred_proba[:, 1]\n",
    "holdout_df.loc[:, 'Model_Prob_Win'] = y_holdout_pred_proba[:, 2]\n",
    "\n",
    "\n",
    "\n",
    "# Step 7: Merge with betting site odds (from df_pivot)\n",
    "\n",
    "\n",
    "holdout_df = holdout_df.merge(odds_df, on='id', how='left')\n",
    "\n",
    "\n",
    "# Fix relative difference calculations based on team location\n",
    "holdout_df['Rel_Diff_Win'] = holdout_df.apply(\n",
    "    lambda row: row['Model_Prob_Win'] - (row['home_win'] if row['home/away'] == 1 else row['away_win']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "holdout_df['Rel_Diff_Loss'] = holdout_df.apply(\n",
    "    lambda row: row['Model_Prob_Loss'] - (row['away_win'] if row['home/away'] == 1 else row['home_win']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "holdout_df['Rel_Diff_Draw'] = holdout_df['Model_Prob_Draw'] - holdout_df['draw']\n",
    "\n",
    "\n",
    "def get_best_bet(row, base_bet=100, threshold=0.05):\n",
    "    rel_diffs = {\n",
    "        'Win': row['Rel_Diff_Win'],\n",
    "        'Loss': row['Rel_Diff_Loss'],\n",
    "        'Draw': row['Rel_Diff_Draw']\n",
    "    }\n",
    "    best_outcome = max(rel_diffs, key=rel_diffs.get)\n",
    "    relative_diff = rel_diffs[best_outcome]\n",
    "    bet_amount = base_bet if relative_diff > threshold else 0\n",
    "    return best_outcome, bet_amount\n",
    "\n",
    "\n",
    "\n",
    "# Apply betting logic to each fixture\n",
    "bets = holdout_df.apply(get_best_bet, axis=1)\n",
    "holdout_df['Bet_Placed'] = bets.apply(lambda x: x[0])\n",
    "holdout_df['Bet_Amount'] = bets.apply(lambda x: x[1])\n",
    "\n",
    "# Step 10: Calculate returns for each fixture\n",
    "holdout_df['Return'] = 0\n",
    "holdout_df['Profit/Loss'] = 0  # Track profit or loss for each game\n",
    "# print(holdout_df.head())\n",
    "count = 0\n",
    "holdout_df['Return'] = 0.0\n",
    "holdout_df['Profit/Loss'] = 0.0\n",
    "\n",
    "for index, row in holdout_df.iterrows():\n",
    "    if row['Bet_Placed'] == 'Win':\n",
    "        odds = 1 / row['home_win']\n",
    "    elif row['Bet_Placed'] == 'Loss':\n",
    "        odds = 1 / row['away_win']\n",
    "    elif row['Bet_Placed'] == 'Draw':\n",
    "        odds = 1 / row['draw']\n",
    "    else:\n",
    "        odds = 0  # No bet placed\n",
    "        count += 1\n",
    "\n",
    "    # Check if the bet matches the actual outcome\n",
    "    actual_outcome = {2: 'Win', 1: 'Draw', 0: 'Loss'}[row['outcome']]\n",
    "    is_successful_bet = row['Bet_Placed'] == actual_outcome\n",
    "\n",
    "\n",
    "    # Calculate return and profit/loss\n",
    "    holdout_df.loc[index, 'Return'] = row['Bet_Amount'] * odds if is_successful_bet else 0\n",
    "    holdout_df.loc[index, 'Profit/Loss'] = holdout_df.loc[index, 'Return'] - row['Bet_Amount']\n",
    "\n",
    "# Step 11: Calculate total profit/loss from all bets\n",
    "total_profit = holdout_df['Profit/Loss'].sum()\n",
    "# print(f\"Total profit/loss from betting: ${total_profit:.2f}\")\n",
    "\n",
    "# Step 12: Save detailed betting results to a CSV file\n",
    "holdout_df.to_csv(\"betting_results2.csv\", index=False)\n",
    "# print(\"Betting results saved to 'betting_results2.csv'\")\n",
    "condensed_checker_df=holdout_df[['season', 'id', 'gwnum', 'name', 'start', 'outcome', 'Model_Prob_Loss', 'Model_Prob_Draw', 'Model_Prob_Win', 'away_win', 'draw', 'home_win', 'Bet_Placed', 'Return', 'Profit/Loss']]\n",
    "condensed_checker_df = condensed_checker_df.sort_values(by = ['id', 'start'])\n",
    "condensed_checker_df.to_csv(\"play_checker.csv\", index=False)\n",
    "# print(condensed_checker_df['profit'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57ee0030-d136-49b8-b133-cb7d086684e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature weights for each outcome class:\n",
      "                                     Loss      Draw       Win\n",
      "gd                              -0.090980 -0.063224  0.154204\n",
      "form_team_prev_gw                0.051787 -0.006268 -0.045518\n",
      "sum_smaller_gw_team_prev_gw     -0.396058  0.016431  0.379628\n",
      "sum_smaller_gw_opponent_prev_gw  0.406427 -0.049427 -0.356999\n",
      "home/away                        0.000000  0.000000  0.000000\n",
      "home_strength_feature2          -0.039051  0.024046  0.015005\n",
      "home_strength_feature           -0.034802  0.033978  0.000824\n",
      "Total profit/loss from betting: $349.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/var/folders/7r/gd_3jt3n5lqcwpyx538q_jjw0000gn/T/ipykernel_48389/2488344692.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  holdout_df['Model_Prob_Loss'] = y_holdout_pred_proba[:, 0]\n",
      "/var/folders/7r/gd_3jt3n5lqcwpyx538q_jjw0000gn/T/ipykernel_48389/2488344692.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  holdout_df['Model_Prob_Draw'] = y_holdout_pred_proba[:, 1]\n",
      "/var/folders/7r/gd_3jt3n5lqcwpyx538q_jjw0000gn/T/ipykernel_48389/2488344692.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  holdout_df['Model_Prob_Win'] = y_holdout_pred_proba[:, 2]\n",
      "/var/folders/7r/gd_3jt3n5lqcwpyx538q_jjw0000gn/T/ipykernel_48389/2488344692.py:161: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '582.2981366459627' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  holdout_df.loc[index, 'Return'] = row['Bet_Amount'] * odds if is_successful_bet else 0\n",
      "/var/folders/7r/gd_3jt3n5lqcwpyx538q_jjw0000gn/T/ipykernel_48389/2488344692.py:162: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '482.2981366459627' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  holdout_df.loc[index, 'Profit/Loss'] = holdout_df.loc[index, 'Return'] - row['Bet_Amount']\n"
     ]
    }
   ],
   "source": [
    "# BETTING SIMULATOR CORRECT(Only looking at home team)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Prepare the data (already partially done)\n",
    "df = combined_df.copy()\n",
    "\n",
    "# Convert 'points' into three classes: 2 = win (3 points), 1 = draw (1 point), 0 = loss (0 points)\n",
    "df['outcome'] = df['points']  # Use 'points' directly (3 -> win, 1 -> draw, 0 -> loss)\n",
    "df['outcome'] = df['points'].map({3: 2, 1: 1, 0: 0})  # Map 3 to 2, 1 to 1, and 0 to 0\n",
    "\n",
    "\n",
    "# Encode 'home/away' as a binary variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['home/away'] = label_encoder.fit_transform(df['home/away'])\n",
    "df = df[df['home/away'] == 1]  # Only keep rows for home team\n",
    "\n",
    "# Add home-specific features\n",
    "df['home_strength_feature'] = df.apply(lambda row: row['home_strength_score'] if row['home/away'] == 1 else 0, axis=1)\n",
    "df['home_strength_feature2'] = df.apply(lambda row: row['home_strength_score2'] if row['home/away'] == 1 else 0, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# 11   17420  2020-09-12            11\n",
    "# 12   18378  2021-08-13            12\n",
    "# 13   19734  2022-08-05            13\n",
    "# 14   21646\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Split into training and holdout sets\n",
    "# Exclude specified seasons from training\n",
    "# exclude_seasons = [18378, 21646, 19734]  # List of seasons to exclude\n",
    "exclude_seasons = [21646]  # List of seasons to exclude\n",
    "df = df[~df['season'].isin(exclude_seasons)].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# exclude_seasons = [18378]  # List of seasons to exclude\n",
    "# df = df[~df['season'].isin(exclude_seasons)].copy()\n",
    "# season_of_interest = 17420  # Future season for testing\n",
    "season_of_interest = 19734  # Future season for testing\n",
    "\n",
    "\n",
    "# exclude_seasons = [18378, 17420]  # List of seasons to exclude\n",
    "# df = df[~df['season'].isin(exclude_seasons)].copy()\n",
    "# season_of_interest = 21646  # Future season for testing\n",
    "# season_of_interest = 18378  # Future season for testing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "holdout_df = df[df['season'] == season_of_interest]  # Test data\n",
    "train_df = df[df['season'] != season_of_interest]  # Training data\n",
    "\n",
    "# Step 3: Define features and target for model training\n",
    "# features = ['gd', 'form_team_prev_gw', 'sum_smaller_gw_team_prev_gw', 'sum_smaller_gw_opponent_prev_gw', 'home/away', 'home_strength_feature2', 'home_strength_feature', 'opponent_form']\n",
    "features = ['gd', 'form_team_prev_gw', 'sum_smaller_gw_team_prev_gw', 'sum_smaller_gw_opponent_prev_gw', 'home/away', 'home_strength_feature2', 'home_strength_feature']\n",
    "X_train = train_df[features]\n",
    "y_train = train_df['outcome']\n",
    "\n",
    "# Step 4: Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Step 5: Train the multinomial logistic regression model\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# View feature weights (coefficients) for each class (Loss, Draw, Win)\n",
    "import pandas as pd\n",
    "\n",
    "feature_weights_df = pd.DataFrame(\n",
    "    model.coef_,\n",
    "    columns=features,\n",
    "    index=['Loss', 'Draw', 'Win']\n",
    ")\n",
    "\n",
    "print(\"Feature weights for each outcome class:\")\n",
    "print(feature_weights_df.T)  # Transpose for readability\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Step 6: Predict probabilities on the holdout set\n",
    "X_holdout = holdout_df[features]\n",
    "y_holdout = holdout_df['outcome']\n",
    "X_holdout_scaled = scaler.transform(X_holdout)\n",
    "\n",
    "# Predict probabilities for win, draw, and loss\n",
    "y_holdout_pred_proba = model.predict_proba(X_holdout_scaled)\n",
    "holdout_df['Model_Prob_Loss'] = y_holdout_pred_proba[:, 0]\n",
    "holdout_df['Model_Prob_Draw'] = y_holdout_pred_proba[:, 1]\n",
    "holdout_df['Model_Prob_Win'] = y_holdout_pred_proba[:, 2]\n",
    "\n",
    "\n",
    "# Step 7: Merge with betting site odds (from df_pivot)\n",
    "\n",
    "\n",
    "holdout_df = holdout_df.merge(odds_df, on='id', how='left')\n",
    "\n",
    "\n",
    "# Step 8: Calculate relative differences\n",
    "holdout_df['Rel_Diff_Home'] = holdout_df['Model_Prob_Win'] - holdout_df['home_win']\n",
    "holdout_df['Rel_Diff_Away'] = holdout_df['Model_Prob_Loss'] - holdout_df['away_win']\n",
    "holdout_df['Rel_Diff_Draw'] = holdout_df['Model_Prob_Draw'] - holdout_df['draw']\n",
    "\n",
    "def get_best_bet(row, base_bet=100, threshold=0.15):\n",
    "    # Calculate relative differences\n",
    "    rel_diffs = {\n",
    "        'Win': row['Rel_Diff_Home'],\n",
    "        'Loss': row['Rel_Diff_Away'],\n",
    "        'Draw': row['Rel_Diff_Draw']\n",
    "    }\n",
    "    # Get the best outcome and relative difference\n",
    "    best_outcome = max(rel_diffs, key=rel_diffs.get)\n",
    "    relative_diff = rel_diffs[best_outcome]\n",
    "    \n",
    "    # Place a bet only if relative difference exceeds the threshold\n",
    "    bet_amount = base_bet if relative_diff > threshold else 0\n",
    "    \n",
    "    return best_outcome, bet_amount\n",
    "\n",
    "\n",
    "# Apply betting logic to each fixture\n",
    "bets = holdout_df.apply(get_best_bet, axis=1)\n",
    "holdout_df['Bet_Placed'] = bets.apply(lambda x: x[0])\n",
    "holdout_df['Bet_Amount'] = bets.apply(lambda x: x[1])\n",
    "\n",
    "# Step 10: Calculate returns for each fixture\n",
    "holdout_df['Return'] = 0\n",
    "holdout_df['Profit/Loss'] = 0  # Track profit or loss for each game\n",
    "# print(holdout_df.head())\n",
    "count = 0\n",
    "for index, row in holdout_df.iterrows():\n",
    "    if row['Bet_Placed'] == 'Win':\n",
    "        odds = 1 / row['home_win']\n",
    "    elif row['Bet_Placed'] == 'Loss':\n",
    "        odds = 1 / row['away_win']\n",
    "    elif row['Bet_Placed'] == 'Draw':\n",
    "        odds = 1 / row['draw']\n",
    "    else:\n",
    "        odds = 0  # No bet placed\n",
    "        count += 1\n",
    "\n",
    "    # Check if the bet matches the actual outcome\n",
    "    actual_outcome = {2: 'Win', 1: 'Draw', 0: 'Loss'}[row['outcome']]\n",
    "    is_successful_bet = row['Bet_Placed'] == actual_outcome\n",
    "\n",
    "\n",
    "    # Calculate return and profit/loss\n",
    "    holdout_df.loc[index, 'Return'] = row['Bet_Amount'] * odds if is_successful_bet else 0\n",
    "    holdout_df.loc[index, 'Profit/Loss'] = holdout_df.loc[index, 'Return'] - row['Bet_Amount']\n",
    "\n",
    "# Step 11: Calculate total profit/loss from all bets\n",
    "total_profit = holdout_df['Profit/Loss'].sum()\n",
    "print(f\"Total profit/loss from betting: ${total_profit:.2f}\")\n",
    "\n",
    "# Step 12: Save detailed betting results to a CSV file\n",
    "holdout_df.to_csv(\"betting_results2.csv\", index=False)\n",
    "# print(\"Betting results saved to 'betting_results2.csv'\")\n",
    "condensed_checker_df=holdout_df[['season', 'id', 'gwnum', 'name', 'start', 'outcome', 'Model_Prob_Loss', 'Model_Prob_Draw', 'Model_Prob_Win', 'away_win', 'draw', 'home_win', 'Bet_Placed', 'Return', 'Profit/Loss']]\n",
    "condensed_checker_df = condensed_checker_df.sort_values(by = ['id', 'start'])\n",
    "condensed_checker_df.to_csv(\"play_checker.csv\", index=False)\n",
    "# print(condensed_checker_df['profit'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3466aa-b44b-4242-b15c-5c83e46bf25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
